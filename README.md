Задание: написать прототип пайплайна анализа лица по видео. На вход подаётся короткий видеоролик с одним человеком, плюс небольшая галерея “известных лиц” (3–5 изображений с именами), которую ты кодируешь в эмбеддинги и используешь как базу сравнения. Для идентификации личности нужно применить любую готовую библиотеку face recognition на базе dlib (подобные библиотеки извлекают face embeddings и достигают точности порядка ~99% на LFW, то есть умеют различать людей по лицу с высокой точностью). Для каждого кадра видео надо извлечь facial landmarks и оценить позу головы (yaw/pitch/roll): это можно сделать через MediaPipe Face Mesh, который в реальном времени предсказывает ~468 3D-точек лица даже на обычной камере, или через OpenFace, который помимо ключевых точек сразу даёт head pose и трекинг лица с вебкамеры без специального железа. Из области рта (ROI по губам) нужно получить гипотезу произносимой фразы с помощью готовой модели визуального чтения по губам в духе LipNet, то есть чисто по видео без аудио; такие модели используют 3D-CNN + рекуррентную часть и CTC, чтобы восстанавливать текст по движению губ. На выходе ты формируешь один JSON с полями по кадрам: identity (имя из галереи или "unknown"), landmarks (координаты ключевых точек), head_pose (yaw/pitch/roll), и отдельно по всему ролику lip_text (распознанная фраза).